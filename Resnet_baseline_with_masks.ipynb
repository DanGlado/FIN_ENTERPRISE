{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "5e762cb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e762cb2",
        "outputId": "82f41d3e-c65d-4267-ab8d-dc4d4965737a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import imageio\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NatureDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,df,transform=None,background=False):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.background = background\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        pic = self.df.iloc[index][\"pic\"]\n",
        "        ans = self.df.iloc[index][\"answers\"]\n",
        "        #print(pic)\n",
        "        pic = np.array(Image.open(pic).convert(\"RGB\"))\n",
        "        \n",
        "        if self.background == True:\n",
        "            mask = self.df.iloc[index][\"mask\"]\n",
        "            mask = np.array(Image.open(mask).convert(\"RGB\"))\n",
        "\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(pic)\n",
        "            if self.background == True:\n",
        "              augmentations = augmentations * self.transform(mask != 0)\n",
        "\n",
        "\n",
        "            pic = augmentations\n",
        "\n",
        "        return pic,ans\n",
        "\n",
        "model = resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(in_features=512, out_features=3)\n",
        "model.load_state_dict(torch.load(\"model.pt\"))\n",
        "model.eval()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.array()\n",
        "np.array()"
      ],
      "metadata": {
        "id": "7BDyYdXjDPUx"
      },
      "id": "7BDyYdXjDPUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a02bbe",
      "metadata": {
        "id": "20a02bbe",
        "outputId": "918702f4-eca5-4e1e-ccf5-9a9b914f90aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pic</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C:\\Users\\ga232\\Природа\\root\\klikun\\images1.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C:\\Users\\ga232\\Природа\\root\\klikun\\images12119...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C:\\Users\\ga232\\Природа\\root\\klikun\\images12130...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C:\\Users\\ga232\\Природа\\root\\klikun\\images12277...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C:\\Users\\ga232\\Природа\\root\\klikun\\images12277...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 pic  answers\n",
              "0     C:\\Users\\ga232\\Природа\\root\\klikun\\images1.jpg        0\n",
              "1  C:\\Users\\ga232\\Природа\\root\\klikun\\images12119...        0\n",
              "2  C:\\Users\\ga232\\Природа\\root\\klikun\\images12130...        0\n",
              "3  C:\\Users\\ga232\\Природа\\root\\klikun\\images12277...        0\n",
              "4  C:\\Users\\ga232\\Природа\\root\\klikun\\images12277...        0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e19e37d",
      "metadata": {
        "id": "1e19e37d",
        "outputId": "6b9e69d8-2403-430d-b473-e259f07abe87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9000,)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answers.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8Kw0Rv8zN6u",
        "outputId": "6ec6c157-1868-4419-e715-e17fd5c468f7"
      },
      "id": "W8Kw0Rv8zN6u",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b58053f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b58053f0",
        "outputId": "3685e9bc-5178-4372-ef08-1fe313602cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                pic            mask\n",
            "0             1.jpg           1.png\n",
            "1    1211999571.jpg  1211999571.png\n",
            "2    1213000553.jpg  1213000553.png\n",
            "3    1227728011.jpg  1227728011.png\n",
            "4    1227728682.jpg  1227728682.png\n",
            "..              ...             ...\n",
            "895      img_95.jpg      img_95.png\n",
            "896      img_96.jpg      img_96.png\n",
            "897      img_97.jpg      img_97.png\n",
            "898      img_98.jpg      img_98.png\n",
            "899      img_99.jpg      img_99.png\n",
            "\n",
            "[900 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-1da14b1985b7>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"pic\"].iloc[:300] = df[\"pic\"].iloc[:300].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/images/кликун/{x}')\n",
            "<ipython-input-33-1da14b1985b7>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"mask\"].iloc[:300] = df[\"mask\"].iloc[:300].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/masks/кликун/{x}')\n",
            "<ipython-input-33-1da14b1985b7>:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"pic\"].iloc[300:600] = df[\"pic\"].iloc[300:600].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/images/малый1/{x}')\n",
            "<ipython-input-33-1da14b1985b7>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"mask\"].iloc[300:600] = df[\"mask\"].iloc[300:600].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/masks/малый1/{x}')\n",
            "<ipython-input-33-1da14b1985b7>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"pic\"].iloc[600:] = df[\"pic\"].iloc[600:].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/images/шипун1/{(x)}')\n",
            "<ipython-input-33-1da14b1985b7>:36: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"mask\"].iloc[600:] = df[\"mask\"].iloc[600:].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/masks/шипун1/{(x)}')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0      /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 1      /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 2      /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 3      /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 4      /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              "                              ...                        \n",
              " 895    /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 896    /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 897    /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 898    /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " 899    /content/gdrive/MyDrive/лебеди_детекция/images...\n",
              " Name: pic, Length: 900, dtype: object,\n",
              " 0      /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 1      /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 2      /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 3      /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 4      /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              "                              ...                        \n",
              " 895    /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 896    /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 897    /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 898    /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " 899    /content/gdrive/MyDrive/лебеди_детекция/masks/...\n",
              " Name: mask, Length: 900, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "df = pd.DataFrame()\n",
        "# кликун - 0, малой -1 , шипун - 2\n",
        "#path1 = r\"C:\\Users\\galina\\лебеди_детекция\\images\\кликун\"\n",
        "#path2 = r\"C:\\Users\\ga232\\Природа\\root\\Maliy\\images\"\n",
        "#path3 = r\"C:\\Users\\ga232\\Природа\\root\\Shipun\\images\"\n",
        "\n",
        "data_directory = '/content/gdrive/MyDrive/лебеди_детекция'\n",
        "img_path1 = data_directory + '/images/кликун/'\n",
        "mask_path1 = data_directory + '/masks/кликун/'\n",
        "\n",
        "img_path2 = data_directory + '/images/малый1/'\n",
        "mask_path2 = data_directory + '/masks/малый1/'\n",
        "\n",
        "img_path3 = data_directory + '/images/шипун1/'\n",
        "mask_path3 = data_directory + '/masks/шипун1/'\n",
        "\n",
        "#print(len(os.listdir(img_path1)), len(os.listdir(img_path2)), len(os.listdir(img_path3)))\n",
        "\n",
        "df[\"pic\"] = sorted(os.listdir(img_path1))[:300] + sorted(os.listdir(img_path2))[:300] + sorted(os.listdir(img_path3)[:300])\n",
        "df[\"mask\"] = sorted(os.listdir(mask_path1))[:300] + sorted(os.listdir(mask_path2))[:300] + sorted(os.listdir(mask_path3)[:300])\n",
        "\n",
        "print(df)\n",
        "#answers = np.array([0 for i in range(300)])\n",
        "answers = np.append(np.array([0 for i in range(300)]) , np.array([1 for i in range(300)]))\n",
        "answers = np.append(answers,np.array([2 for i in range(300)]))\n",
        "df[\"answers\"] = answers\n",
        "\n",
        "#Справа тоже впиши свой путь\n",
        "df[\"pic\"].iloc[:300] = df[\"pic\"].iloc[:300].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/images/кликун/{x}')\n",
        "df[\"mask\"].iloc[:300] = df[\"mask\"].iloc[:300].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/masks/кликун/{x}')\n",
        "\n",
        "df[\"pic\"].iloc[300:600] = df[\"pic\"].iloc[300:600].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/images/малый1/{x}')\n",
        "df[\"mask\"].iloc[300:600] = df[\"mask\"].iloc[300:600].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/masks/малый1/{x}')\n",
        "\n",
        "df[\"pic\"].iloc[600:] = df[\"pic\"].iloc[600:].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/images/шипун1/{(x)}')\n",
        "df[\"mask\"].iloc[600:] = df[\"mask\"].iloc[600:].apply(lambda x: f'/content/gdrive/MyDrive/лебеди_детекция/masks/шипун1/{(x)}')\n",
        "\n",
        "df[\"pic\"], df['mask']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(os.listdir(img_path3)[:300]))\n",
        "print(sorted(os.listdir(mask_path3)[:300]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdLZS_iYWKYe",
        "outputId": "e88c8bd8-8756-494c-9461-1c8b4594d2f8"
      },
      "id": "vdLZS_iYWKYe",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['img_0.jpg', 'img_1.jpg', 'img_10.jpg', 'img_100.jpg', 'img_101.jpg', 'img_102.jpg', 'img_103.jpg', 'img_104.jpg', 'img_105.jpg', 'img_106.jpg', 'img_107.jpg', 'img_108.jpg', 'img_109.jpg', 'img_11.jpg', 'img_110.jpg', 'img_111.jpg', 'img_112.jpg', 'img_113.jpg', 'img_114.jpg', 'img_115.jpg', 'img_116.jpg', 'img_117.jpg', 'img_118.jpg', 'img_119.jpg', 'img_12.jpg', 'img_120.jpg', 'img_121.jpg', 'img_122.jpg', 'img_123.jpg', 'img_124.jpg', 'img_125.jpg', 'img_126.jpg', 'img_127.jpg', 'img_128.jpg', 'img_129.jpg', 'img_13.jpg', 'img_130.jpg', 'img_131.jpg', 'img_132.jpg', 'img_133.jpg', 'img_134.jpg', 'img_135.jpg', 'img_136.jpg', 'img_137.jpg', 'img_138.jpg', 'img_139.jpg', 'img_14.jpg', 'img_140.jpg', 'img_141.jpg', 'img_142.jpg', 'img_143.jpg', 'img_144.jpg', 'img_145.jpg', 'img_146.jpg', 'img_147.jpg', 'img_148.jpg', 'img_149.jpg', 'img_15.jpg', 'img_150.jpg', 'img_151.jpg', 'img_152.jpg', 'img_153.jpg', 'img_154.jpg', 'img_155.jpg', 'img_156.jpg', 'img_157.jpg', 'img_158.jpg', 'img_159.jpg', 'img_16.jpg', 'img_160.jpg', 'img_161.jpg', 'img_162.jpg', 'img_163.jpg', 'img_164.jpg', 'img_165.jpg', 'img_166.jpg', 'img_167.jpg', 'img_168.jpg', 'img_169.jpg', 'img_17.jpg', 'img_170.jpg', 'img_171.jpg', 'img_172.jpg', 'img_173.jpg', 'img_174.jpg', 'img_175.jpg', 'img_176.jpg', 'img_177.jpg', 'img_178.jpg', 'img_179.jpg', 'img_18.jpg', 'img_180.jpg', 'img_181.jpg', 'img_182.jpg', 'img_183.jpg', 'img_184.jpg', 'img_185.jpg', 'img_186.jpg', 'img_187.jpg', 'img_188.jpg', 'img_189.jpg', 'img_19.jpg', 'img_190.jpg', 'img_191.jpg', 'img_192.jpg', 'img_193.jpg', 'img_194.jpg', 'img_195.jpg', 'img_196.jpg', 'img_197.jpg', 'img_198.jpg', 'img_199.jpg', 'img_2.jpg', 'img_20.jpg', 'img_200.jpg', 'img_201.jpg', 'img_202.jpg', 'img_203.jpg', 'img_204.jpg', 'img_205.jpg', 'img_206.jpg', 'img_207.jpg', 'img_208.jpg', 'img_209.jpg', 'img_21.jpg', 'img_210.jpg', 'img_211.jpg', 'img_212.jpg', 'img_213.jpg', 'img_214.jpg', 'img_215.jpg', 'img_216.jpg', 'img_217.jpg', 'img_218.jpg', 'img_219.jpg', 'img_22.jpg', 'img_220.jpg', 'img_221.jpg', 'img_222.jpg', 'img_223.jpg', 'img_224.jpg', 'img_225.jpg', 'img_226.jpg', 'img_227.jpg', 'img_228.jpg', 'img_229.jpg', 'img_23.jpg', 'img_230.jpg', 'img_231.jpg', 'img_232.jpg', 'img_233.jpg', 'img_234.jpg', 'img_235.jpg', 'img_236.jpg', 'img_237.jpg', 'img_238.jpg', 'img_239.jpg', 'img_24.jpg', 'img_240.jpg', 'img_241.jpg', 'img_242.jpg', 'img_243.jpg', 'img_244.jpg', 'img_245.jpg', 'img_246.jpg', 'img_247.jpg', 'img_248.jpg', 'img_249.jpg', 'img_25.jpg', 'img_250.jpg', 'img_251.jpg', 'img_252.jpg', 'img_253.jpg', 'img_254.jpg', 'img_255.jpg', 'img_256.jpg', 'img_257.jpg', 'img_258.jpg', 'img_259.jpg', 'img_26.jpg', 'img_260.jpg', 'img_261.jpg', 'img_262.jpg', 'img_263.jpg', 'img_264.jpg', 'img_265.jpg', 'img_266.jpg', 'img_267.jpg', 'img_268.jpg', 'img_269.jpg', 'img_27.jpg', 'img_270.jpg', 'img_271.jpg', 'img_272.jpg', 'img_273.jpg', 'img_274.jpg', 'img_275.jpg', 'img_276.jpg', 'img_277.jpg', 'img_278.jpg', 'img_279.jpg', 'img_28.jpg', 'img_280.jpg', 'img_281.jpg', 'img_282.jpg', 'img_283.jpg', 'img_284.jpg', 'img_285.jpg', 'img_286.jpg', 'img_287.jpg', 'img_288.jpg', 'img_289.jpg', 'img_29.jpg', 'img_290.jpg', 'img_291.jpg', 'img_292.jpg', 'img_293.jpg', 'img_294.jpg', 'img_295.jpg', 'img_296.jpg', 'img_297.jpg', 'img_298.jpg', 'img_299.jpg', 'img_3.jpg', 'img_30.jpg', 'img_31.jpg', 'img_32.jpg', 'img_33.jpg', 'img_34.jpg', 'img_35.jpg', 'img_36.jpg', 'img_37.jpg', 'img_38.jpg', 'img_39.jpg', 'img_4.jpg', 'img_40.jpg', 'img_41.jpg', 'img_42.jpg', 'img_43.jpg', 'img_44.jpg', 'img_45.jpg', 'img_46.jpg', 'img_47.jpg', 'img_48.jpg', 'img_49.jpg', 'img_5.jpg', 'img_50.jpg', 'img_51.jpg', 'img_52.jpg', 'img_53.jpg', 'img_54.jpg', 'img_55.jpg', 'img_56.jpg', 'img_57.jpg', 'img_58.jpg', 'img_59.jpg', 'img_6.jpg', 'img_60.jpg', 'img_61.jpg', 'img_62.jpg', 'img_63.jpg', 'img_64.jpg', 'img_65.jpg', 'img_66.jpg', 'img_67.jpg', 'img_68.jpg', 'img_69.jpg', 'img_7.jpg', 'img_70.jpg', 'img_71.jpg', 'img_72.jpg', 'img_73.jpg', 'img_74.jpg', 'img_75.jpg', 'img_76.jpg', 'img_77.jpg', 'img_78.jpg', 'img_79.jpg', 'img_8.jpg', 'img_80.jpg', 'img_81.jpg', 'img_82.jpg', 'img_83.jpg', 'img_84.jpg', 'img_85.jpg', 'img_86.jpg', 'img_87.jpg', 'img_88.jpg', 'img_89.jpg', 'img_9.jpg', 'img_90.jpg', 'img_91.jpg', 'img_92.jpg', 'img_93.jpg', 'img_94.jpg', 'img_95.jpg', 'img_96.jpg', 'img_97.jpg', 'img_98.jpg', 'img_99.jpg']\n",
            "['img_0.png', 'img_1.png', 'img_10.png', 'img_100.png', 'img_101.png', 'img_102.png', 'img_103.png', 'img_104.png', 'img_105.png', 'img_106.png', 'img_107.png', 'img_108.png', 'img_109.png', 'img_11.png', 'img_110.png', 'img_111.png', 'img_112.png', 'img_113.png', 'img_114.png', 'img_115.png', 'img_116.png', 'img_117.png', 'img_118.png', 'img_119.png', 'img_12.png', 'img_120.png', 'img_121.png', 'img_122.png', 'img_123.png', 'img_124.png', 'img_125.png', 'img_126.png', 'img_127.png', 'img_128.png', 'img_129.png', 'img_13.png', 'img_130.png', 'img_131.png', 'img_132.png', 'img_133.png', 'img_134.png', 'img_135.png', 'img_136.png', 'img_137.png', 'img_138.png', 'img_139.png', 'img_14.png', 'img_140.png', 'img_141.png', 'img_142.png', 'img_143.png', 'img_144.png', 'img_145.png', 'img_146.png', 'img_147.png', 'img_148.png', 'img_149.png', 'img_15.png', 'img_150.png', 'img_151.png', 'img_152.png', 'img_153.png', 'img_154.png', 'img_155.png', 'img_156.png', 'img_157.png', 'img_158.png', 'img_159.png', 'img_16.png', 'img_160.png', 'img_161.png', 'img_162.png', 'img_163.png', 'img_164.png', 'img_165.png', 'img_166.png', 'img_167.png', 'img_168.png', 'img_169.png', 'img_17.png', 'img_170.png', 'img_171.png', 'img_172.png', 'img_173.png', 'img_174.png', 'img_175.png', 'img_176.png', 'img_177.png', 'img_178.png', 'img_179.png', 'img_18.png', 'img_180.png', 'img_181.png', 'img_182.png', 'img_183.png', 'img_184.png', 'img_185.png', 'img_186.png', 'img_187.png', 'img_188.png', 'img_189.png', 'img_19.png', 'img_190.png', 'img_191.png', 'img_192.png', 'img_193.png', 'img_194.png', 'img_195.png', 'img_196.png', 'img_197.png', 'img_198.png', 'img_199.png', 'img_2.png', 'img_20.png', 'img_200.png', 'img_201.png', 'img_202.png', 'img_203.png', 'img_204.png', 'img_205.png', 'img_206.png', 'img_207.png', 'img_208.png', 'img_209.png', 'img_21.png', 'img_210.png', 'img_211.png', 'img_212.png', 'img_213.png', 'img_214.png', 'img_215.png', 'img_216.png', 'img_217.png', 'img_218.png', 'img_219.png', 'img_22.png', 'img_220.png', 'img_221.png', 'img_222.png', 'img_223.png', 'img_224.png', 'img_225.png', 'img_226.png', 'img_227.png', 'img_228.png', 'img_229.png', 'img_23.png', 'img_230.png', 'img_231.png', 'img_232.png', 'img_233.png', 'img_234.png', 'img_235.png', 'img_236.png', 'img_237.png', 'img_238.png', 'img_239.png', 'img_24.png', 'img_240.png', 'img_241.png', 'img_242.png', 'img_243.png', 'img_244.png', 'img_245.png', 'img_246.png', 'img_247.png', 'img_248.png', 'img_249.png', 'img_25.png', 'img_250.png', 'img_251.png', 'img_252.png', 'img_253.png', 'img_254.png', 'img_255.png', 'img_256.png', 'img_257.png', 'img_258.png', 'img_259.png', 'img_26.png', 'img_260.png', 'img_261.png', 'img_262.png', 'img_263.png', 'img_264.png', 'img_265.png', 'img_266.png', 'img_267.png', 'img_268.png', 'img_269.png', 'img_27.png', 'img_270.png', 'img_271.png', 'img_272.png', 'img_273.png', 'img_274.png', 'img_275.png', 'img_276.png', 'img_277.png', 'img_278.png', 'img_279.png', 'img_28.png', 'img_280.png', 'img_281.png', 'img_282.png', 'img_283.png', 'img_284.png', 'img_285.png', 'img_286.png', 'img_287.png', 'img_288.png', 'img_289.png', 'img_29.png', 'img_290.png', 'img_291.png', 'img_292.png', 'img_293.png', 'img_294.png', 'img_295.png', 'img_296.png', 'img_297.png', 'img_298.png', 'img_299.png', 'img_3.png', 'img_30.png', 'img_31.png', 'img_32.png', 'img_33.png', 'img_34.png', 'img_35.png', 'img_36.png', 'img_37.png', 'img_38.png', 'img_39.png', 'img_4.png', 'img_40.png', 'img_41.png', 'img_42.png', 'img_43.png', 'img_44.png', 'img_45.png', 'img_46.png', 'img_47.png', 'img_48.png', 'img_49.png', 'img_5.png', 'img_50.png', 'img_51.png', 'img_52.png', 'img_53.png', 'img_54.png', 'img_55.png', 'img_56.png', 'img_57.png', 'img_58.png', 'img_59.png', 'img_6.png', 'img_60.png', 'img_61.png', 'img_62.png', 'img_63.png', 'img_64.png', 'img_65.png', 'img_66.png', 'img_67.png', 'img_68.png', 'img_69.png', 'img_7.png', 'img_70.png', 'img_71.png', 'img_72.png', 'img_73.png', 'img_74.png', 'img_75.png', 'img_76.png', 'img_77.png', 'img_78.png', 'img_79.png', 'img_8.png', 'img_80.png', 'img_81.png', 'img_82.png', 'img_83.png', 'img_84.png', 'img_85.png', 'img_86.png', 'img_87.png', 'img_88.png', 'img_89.png', 'img_9.png', 'img_90.png', 'img_91.png', 'img_92.png', 'img_93.png', 'img_94.png', 'img_95.png', 'img_96.png', 'img_97.png', 'img_98.png', 'img_99.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(img_path1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QENNpK2A_gFr",
        "outputId": "d3d0983f-92d1-45a7-eb09-486480e80f8d"
      },
      "id": "QENNpK2A_gFr",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3016"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5b46e4e2",
      "metadata": {
        "id": "5b46e4e2"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train_dataset, test_dataset = train_test_split(df)\n",
        "\n",
        "\n",
        "transform = T.Compose(\n",
        "    [\n",
        "        T.ToTensor(),\n",
        "        T.Resize((224, 224)),\n",
        "        \n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = NatureDataset(train_dataset,transform =transform)\n",
        "test_dataset = NatureDataset(test_dataset,transform =transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bxgapWaR5ti",
        "outputId": "aa86af5c-10f8-4c2e-9575-88274b5e0174"
      },
      "id": "3bxgapWaR5ti",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.1534, 0.2015, 0.2181,  ..., 0.1129, 0.1836, 0.1931],\n",
              "          [0.1619, 0.1588, 0.2041,  ..., 0.1230, 0.1603, 0.2033],\n",
              "          [0.1800, 0.1635, 0.1929,  ..., 0.1670, 0.1613, 0.1667],\n",
              "          ...,\n",
              "          [0.5849, 0.6015, 0.6008,  ..., 0.6018, 0.6104, 0.6051],\n",
              "          [0.5686, 0.5746, 0.5759,  ..., 0.6050, 0.6069, 0.5960],\n",
              "          [0.5667, 0.5629, 0.5647,  ..., 0.6039, 0.5995, 0.5838]],\n",
              " \n",
              "         [[0.1416, 0.1897, 0.2063,  ..., 0.1124, 0.1786, 0.1813],\n",
              "          [0.1501, 0.1471, 0.1923,  ..., 0.1190, 0.1485, 0.1848],\n",
              "          [0.1682, 0.1518, 0.1811,  ..., 0.1580, 0.1417, 0.1455],\n",
              "          ...,\n",
              "          [0.5928, 0.6094, 0.6086,  ..., 0.6057, 0.6144, 0.6090],\n",
              "          [0.5765, 0.5824, 0.5837,  ..., 0.6089, 0.6108, 0.5999],\n",
              "          [0.5745, 0.5707, 0.5725,  ..., 0.6078, 0.6034, 0.5877]],\n",
              " \n",
              "         [[0.1220, 0.1701, 0.1867,  ..., 0.0911, 0.1590, 0.1617],\n",
              "          [0.1305, 0.1275, 0.1727,  ..., 0.0994, 0.1289, 0.1686],\n",
              "          [0.1486, 0.1321, 0.1615,  ..., 0.1384, 0.1260, 0.1304],\n",
              "          ...,\n",
              "          [0.5754, 0.5976, 0.6047,  ..., 0.6136, 0.6222, 0.6168],\n",
              "          [0.5591, 0.5707, 0.5798,  ..., 0.6168, 0.6186, 0.6078],\n",
              "          [0.5572, 0.5590, 0.5686,  ..., 0.6157, 0.6113, 0.5956]]]),\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d4405a38",
      "metadata": {
        "id": "d4405a38"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, pin_memory=True )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RBbX10mFBpVA"
      },
      "id": "RBbX10mFBpVA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "91fd5e76",
      "metadata": {
        "id": "91fd5e76"
      },
      "outputs": [],
      "source": [
        "x,y = next(iter(train_loader)) # Показываю accuracy на 1 баче, на деле оно похуже естественно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "41e9daa3",
      "metadata": {
        "id": "41e9daa3",
        "outputId": "09d5abf0-951c-45fd-82be-45164a0f7819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9453)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "pred = torch.max(model(x),1)[1] # prediction\n",
        "sum(y==pred)/len(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "764ff6da",
      "metadata": {
        "id": "764ff6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d7465a-0677-4521-b538-08f261e3b336"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
              " 2)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "train_dataset1, test_dataset1 = train_test_split(df)\n",
        "\n",
        "train_dataset_new = NatureDataset(train_dataset1,transform =transform, background=True)\n",
        "test_dataset_new = NatureDataset(test_dataset1,transform =transform, background=True)\n",
        "\n",
        "train_dataset_new[10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader1 = DataLoader(train_dataset_new, batch_size=128, shuffle=True, pin_memory=True)\n",
        "test_loader1 = DataLoader(test_dataset_new, batch_size=128, shuffle=True, pin_memory=True )"
      ],
      "metadata": {
        "id": "8n5qbVfBDn9A"
      },
      "id": "8n5qbVfBDn9A",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(train_loader1)) # Показываю accuracy на 1 баче, на деле оно похуже естественно"
      ],
      "metadata": {
        "id": "p8-jKNZLCO0Y"
      },
      "id": "p8-jKNZLCO0Y",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.max(model(x),1)[1] # prediction\n",
        "sum(y==pred)/len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54KPUUYxE4cr",
        "outputId": "bf3dfaa4-f318-42e9-96b5-b6444e0b68e1"
      },
      "id": "54KPUUYxE4cr",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7578)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}